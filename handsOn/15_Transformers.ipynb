{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 15 Transformers\n",
    "\n",
    "Transformers are a type of deep learning architecture that have revolutionized the field of natural language processing (NLP) due to their remarkable performance on a variety of tasks.\n",
    "\n",
    "Key Characteristics:\n",
    "\n",
    "* _Attention Mechanisms:_ At the heart of Transformers is the self-attention mechanism that can weigh input tokens differently, allowing the model to focus on various parts of the input data. This mechanism can capture relationships between tokens regardless of their positions or distance from one another.\n",
    "\n",
    "* _Parallel Processing:_ Unlike recurrent neural networks (RNNs) that process data sequentially, Transformers process all tokens in the input data in parallel, which leads to significant speed-ups during training.\n",
    "\n",
    "* _Scalability:_ Transformers are highly scalable. This means they can be trained with a vast number of parameters (often billions), leading to models like GPT-3 from OpenAI.\n",
    "\n",
    "* _Positional Encoding:_ Since Transformers don't process data sequentially, they don't have a built-in notion of the order or position of tokens. To address this, positional encodings are added to the embeddings at the input layer, providing the model with positional context.\n",
    "\n",
    "### Transformer Architecture\n",
    "The Transformer model consists of an Encoder-Decoder structure. Both the encoder and the decoder are composed of a stack of identical layers.\n",
    "\n",
    "1. Encoder: The encoder receives the input data (like a sentence) and compresses the information into a 'context' or 'memory' that the decoder can then use. The encoder consists of a stack of identical layers. Each layer has two main components: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n",
    "\n",
    "\n",
    "* _Input Embedding:_ The input data is first converted into vectors using embedding layers. Positional encoding is then added to these embeddings to give the model information about the position of words in a sequence.\n",
    "\n",
    "* _Encoder Stack:_ Multiple (often 6 or more) identical layers are stacked. Each layer has two main components:\n",
    "    (1) Multi-Head Attention Mechanism: This allows the encoder to focus on different parts of the input sentence when producing the context. It uses the attention mechanism we discussed earlier but multiple times in parallel.\n",
    "    (2) Feed-Forward Neural Network: Each attention output is then passed through a feed-forward neural network (the same one for each position).\n",
    "\n",
    "2. Decoder: The decoder generates the output data (like the translation of the input sentence) from the context provided by the encoder. Also consists of a stack of identical layers. In addition to the two components in the encoder layer, the decoder has a third component, which is a multi-head attention over the encoder's output.\n",
    "\n",
    "* _Output Embedding:_ Like the input embedding but for the output data.\n",
    "\n",
    "* _Decoder Stack:_ Also composed of multiple identical layers. Each layer has three main components:\n",
    "(1) Masked Multi-Head Attention Mechanism: This ensures that the prediction for a particular word doesn’t depend on future words in the sequence. It's \"masked\" to prevent the model from \"cheating\" by looking ahead.\n",
    "(2) Multi-Head Attention Mechanism (over encoder’s output): This helps the decoder focus on relevant parts of the input sentence, much like in the encoder but attending to the encoder's output.\n",
    "(3) Feed-Forward Neural Network: Just like in the encoder.\n",
    "\n",
    "The final layer of the decoder produces the output sequence."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "model_name = 'Helsinki-NLP/opus-mt-en-es'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: MarianEncoder(\n",
      "  (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "  (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x MarianEncoderLayer(\n",
      "      (self_attn): MarianAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): SiLUActivation()\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Decoder: MarianDecoder(\n",
      "  (embed_tokens): Embedding(65001, 512, padding_idx=65000)\n",
      "  (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
      "  (layers): ModuleList(\n",
      "    (0-5): 6 x MarianDecoderLayer(\n",
      "      (self_attn): MarianAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (activation_fn): SiLUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): MarianAttention(\n",
      "        (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "      (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = model.get_encoder()\n",
    "print(\"Encoder:\", encoder)\n",
    "decoder = model.get_decoder()\n",
    "print(\"Decoder:\", decoder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def translate_to_spanish(phrase):\n",
    "    \"\"\"Translate an English phrase to Spanish using the pre-trained Transformer model.\"\"\"\n",
    "    # Tokenize and encode the phrase\n",
    "    encoded_phrase = tokenizer.encode(phrase, return_tensors=\"pt\")  # Use \"tf\" for TensorFlow\n",
    "    # Generate the translation from the encoded phrase\n",
    "    translation_ids = model.generate(encoded_phrase)\n",
    "    # Convert token IDs back to a string\n",
    "    translation = tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
    "    return translation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\handson310\\lib\\site-packages\\transformers\\generation\\utils.py:1369: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el rápido zorro marrón salta sobre el perro perezoso\n"
     ]
    }
   ],
   "source": [
    "print(translate_to_spanish(\"the quick brown fox jumps over the lazy dog\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
